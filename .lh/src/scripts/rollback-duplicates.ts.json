{
    "sourceFile": "src/scripts/rollback-duplicates.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 19,
            "patches": [
                {
                    "date": 1739791839926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1739791905133,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,8 @@\n import { createClient } from '@supabase/supabase-js';\r\n import dotenv from 'dotenv';\r\n import path from 'path';\r\n+import { fileURLToPath } from 'url';\r\n \r\n // Load environment variables\r\n dotenv.config({ path: path.resolve(process.cwd(), '.env') });\r\n \r\n@@ -65,10 +66,10 @@\n     throw error;\r\n   }\r\n }\r\n \r\n-// Execute if running directly\r\n-if (require.main === module) {\r\n+// Execute if this is the main module\r\n+if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n   rollbackDuplicates()\r\n     .then(() => process.exit(0))\r\n     .catch((error) => {\r\n       console.error(error);\r\n"
                },
                {
                    "date": 1739792005192,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n const supabase = createClient(supabaseUrl, supabaseKey);\r\n \r\n async function rollbackDuplicates() {\r\n   console.log('Starting rollback of duplicate entries...');\r\n+  console.log('Using Supabase URL:', supabaseUrl);\r\n \r\n   try {\r\n     // First, identify duplicates by original_id\r\n     const { data: duplicates, error: findError } = await supabase\r\n@@ -25,12 +26,22 @@\n       .select('original_id, id, created_at')\r\n       .not('original_id', 'is', null)\r\n       .order('created_at', { ascending: false });\r\n \r\n-    if (findError) throw findError;\r\n+    if (findError) {\r\n+      console.error('Error finding duplicates:', findError);\r\n+      throw findError;\r\n+    }\r\n \r\n+    if (!duplicates || duplicates.length === 0) {\r\n+      console.log('No questions found with original_id.');\r\n+      return;\r\n+    }\r\n+\r\n+    console.log(`Found ${duplicates.length} total questions with original_id`);\r\n+\r\n     // Group by original_id and keep only the latest entry\r\n-    const groupedByOriginalId = duplicates?.reduce((acc, curr) => {\r\n+    const groupedByOriginalId = duplicates.reduce((acc, curr) => {\r\n       if (!acc[curr.original_id]) {\r\n         acc[curr.original_id] = [];\r\n       }\r\n       acc[curr.original_id].push(curr.id);\r\n@@ -49,15 +60,24 @@\n     }\r\n \r\n     console.log(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n \r\n-    // Delete duplicates\r\n-    const { error: deleteError } = await supabase\r\n-      .from('questions')\r\n-      .delete()\r\n-      .in('id', idsToDelete);\r\n+    // Delete duplicates in batches of 100\r\n+    const batchSize = 100;\r\n+    for (let i = 0; i < idsToDelete.length; i += batchSize) {\r\n+      const batch = idsToDelete.slice(i, i + batchSize);\r\n+      console.log(`Deleting batch ${i / batchSize + 1}...`);\r\n+      \r\n+      const { error: deleteError } = await supabase\r\n+        .from('questions')\r\n+        .delete()\r\n+        .in('id', batch);\r\n \r\n-    if (deleteError) throw deleteError;\r\n+      if (deleteError) {\r\n+        console.error('Error deleting batch:', deleteError);\r\n+        throw deleteError;\r\n+      }\r\n+    }\r\n \r\n     console.log('Successfully removed duplicate entries.');\r\n     console.log(`Removed ${idsToDelete.length} duplicates.`);\r\n \r\n@@ -68,12 +88,16 @@\n }\r\n \r\n // Execute if this is the main module\r\n if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n+  console.log('Starting script...');\r\n   rollbackDuplicates()\r\n-    .then(() => process.exit(0))\r\n+    .then(() => {\r\n+      console.log('Script completed successfully');\r\n+      process.exit(0);\r\n+    })\r\n     .catch((error) => {\r\n-      console.error(error);\r\n+      console.error('Script failed:', error);\r\n       process.exit(1);\r\n     });\r\n }\r\n \r\n"
                },
                {
                    "date": 1739794014471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,119 @@\n+import { createClient } from '@supabase/supabase-js';\r\n+import dotenv from 'dotenv';\r\n+import path from 'path';\r\n+import { fileURLToPath } from 'url';\r\n+\r\n+// Load environment variables\r\n+dotenv.config({ path: path.resolve(process.cwd(), '.env') });\r\n+\r\n+const supabaseUrl = process.env.VITE_SUPABASE_URL;\r\n+const supabaseKey = process.env.SUPABASE_SERVICE_KEY;\r\n+\r\n+if (!supabaseUrl || !supabaseKey) {\r\n+  throw new Error('Missing Supabase environment variables');\r\n+}\r\n+\r\n+const supabase = createClient(supabaseUrl, supabaseKey);\r\n+\r\n+async function rollbackDuplicates() {\r\n+  console.log('Starting rollback of duplicate entries...');\r\n+\r\n+  try {\r\n+    // Get all questions to check their structure\r\n+    const { data: sampleQuestion, error: sampleError } = await supabase\r\n+      .from('questions')\r\n+      .select('*')\r\n+      .limit(1)\r\n+      .single();\r\n+\r\n+    if (sampleError) {\r\n+      console.error('Error fetching sample question:', sampleError);\r\n+      throw sampleError;\r\n+    }\r\n+\r\n+    console.log('Sample question structure:', sampleQuestion);\r\n+\r\n+    // Now proceed with duplicate check\r\n+    const { data: duplicates, error: findError } = await supabase\r\n+      .from('questions')\r\n+      .select('*')\r\n+      .not('original_id', 'is', null);\r\n+\r\n+    if (findError) {\r\n+      console.error('Error finding duplicates:', findError);\r\n+      throw findError;\r\n+    }\r\n+\r\n+    if (!duplicates || duplicates.length === 0) {\r\n+      console.log('No questions found.');\r\n+      return;\r\n+    }\r\n+\r\n+    console.log(`Total questions found: ${duplicates.length}`);\r\n+\r\n+    // Log some sample data to verify structure\r\n+    console.log('Sample entries:', duplicates.slice(0, 2));\r\n+\r\n+    // Group by original_id and keep only the latest entry\r\n+    const groupedByOriginalId = duplicates.reduce((acc, curr) => {\r\n+      if (!acc[curr.original_id]) {\r\n+        acc[curr.original_id] = [];\r\n+      }\r\n+      acc[curr.original_id].push(curr.id);\r\n+      return acc;\r\n+    }, {} as Record<string, string[]>);\r\n+\r\n+    // Get IDs to delete (all except the first/latest one for each original_id)\r\n+    const idsToDelete = Object.values(groupedByOriginalId)\r\n+      .filter(ids => ids.length > 1)\r\n+      .map(ids => ids.slice(1))\r\n+      .flat();\r\n+\r\n+    if (idsToDelete.length === 0) {\r\n+      console.log('No duplicates found.');\r\n+      return;\r\n+    }\r\n+\r\n+    console.log(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n+\r\n+    // Delete duplicates in batches of 100\r\n+    const batchSize = 100;\r\n+    for (let i = 0; i < idsToDelete.length; i += batchSize) {\r\n+      const batch = idsToDelete.slice(i, i + batchSize);\r\n+      console.log(`Deleting batch ${i / batchSize + 1}...`);\r\n+      \r\n+      const { error: deleteError } = await supabase\r\n+        .from('questions')\r\n+        .delete()\r\n+        .in('id', batch);\r\n+\r\n+      if (deleteError) {\r\n+        console.error('Error deleting batch:', deleteError);\r\n+        throw deleteError;\r\n+      }\r\n+    }\r\n+\r\n+    console.log('Successfully removed duplicate entries.');\r\n+    console.log(`Removed ${idsToDelete.length} duplicates.`);\r\n+\r\n+  } catch (error) {\r\n+    console.error('Error during rollback:', error);\r\n+    throw error;\r\n+  }\r\n+}\r\n+\r\n+// Execute if this is the main module\r\n+if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n+  console.log('Starting script...');\r\n+  rollbackDuplicates()\r\n+    .then(() => {\r\n+      console.log('Script completed successfully');\r\n+      process.exit(0);\r\n+    })\r\n+    .catch((error) => {\r\n+      console.error('Script failed:', error);\r\n+      process.exit(1);\r\n+    });\r\n+}\r\n+\r\n+export { rollbackDuplicates }; \n\\ No newline at end of file\n"
                },
                {
                    "date": 1739794020396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,109 +115,5 @@\n       process.exit(1);\r\n     });\r\n }\r\n \r\n-export { rollbackDuplicates }; \n-import { createClient } from '@supabase/supabase-js';\r\n-import dotenv from 'dotenv';\r\n-import path from 'path';\r\n-import { fileURLToPath } from 'url';\r\n-\r\n-// Load environment variables\r\n-dotenv.config({ path: path.resolve(process.cwd(), '.env') });\r\n-\r\n-const supabaseUrl = process.env.VITE_SUPABASE_URL;\r\n-const supabaseKey = process.env.SUPABASE_SERVICE_KEY;\r\n-\r\n-if (!supabaseUrl || !supabaseKey) {\r\n-  throw new Error('Missing Supabase environment variables');\r\n-}\r\n-\r\n-const supabase = createClient(supabaseUrl, supabaseKey);\r\n-\r\n-async function rollbackDuplicates() {\r\n-  console.log('Starting rollback of duplicate entries...');\r\n-  console.log('Using Supabase URL:', supabaseUrl);\r\n-\r\n-  try {\r\n-    // First, identify duplicates by original_id\r\n-    const { data: duplicates, error: findError } = await supabase\r\n-      .from('questions')\r\n-      .select('original_id, id, created_at')\r\n-      .not('original_id', 'is', null)\r\n-      .order('created_at', { ascending: false });\r\n-\r\n-    if (findError) {\r\n-      console.error('Error finding duplicates:', findError);\r\n-      throw findError;\r\n-    }\r\n-\r\n-    if (!duplicates || duplicates.length === 0) {\r\n-      console.log('No questions found with original_id.');\r\n-      return;\r\n-    }\r\n-\r\n-    console.log(`Found ${duplicates.length} total questions with original_id`);\r\n-\r\n-    // Group by original_id and keep only the latest entry\r\n-    const groupedByOriginalId = duplicates.reduce((acc, curr) => {\r\n-      if (!acc[curr.original_id]) {\r\n-        acc[curr.original_id] = [];\r\n-      }\r\n-      acc[curr.original_id].push(curr.id);\r\n-      return acc;\r\n-    }, {} as Record<string, string[]>);\r\n-\r\n-    // Get IDs to delete (all except the first/latest one for each original_id)\r\n-    const idsToDelete = Object.values(groupedByOriginalId)\r\n-      .filter(ids => ids.length > 1)\r\n-      .map(ids => ids.slice(1))\r\n-      .flat();\r\n-\r\n-    if (idsToDelete.length === 0) {\r\n-      console.log('No duplicates found.');\r\n-      return;\r\n-    }\r\n-\r\n-    console.log(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n-\r\n-    // Delete duplicates in batches of 100\r\n-    const batchSize = 100;\r\n-    for (let i = 0; i < idsToDelete.length; i += batchSize) {\r\n-      const batch = idsToDelete.slice(i, i + batchSize);\r\n-      console.log(`Deleting batch ${i / batchSize + 1}...`);\r\n-      \r\n-      const { error: deleteError } = await supabase\r\n-        .from('questions')\r\n-        .delete()\r\n-        .in('id', batch);\r\n-\r\n-      if (deleteError) {\r\n-        console.error('Error deleting batch:', deleteError);\r\n-        throw deleteError;\r\n-      }\r\n-    }\r\n-\r\n-    console.log('Successfully removed duplicate entries.');\r\n-    console.log(`Removed ${idsToDelete.length} duplicates.`);\r\n-\r\n-  } catch (error) {\r\n-    console.error('Error during rollback:', error);\r\n-    throw error;\r\n-  }\r\n-}\r\n-\r\n-// Execute if this is the main module\r\n-if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n-  console.log('Starting script...');\r\n-  rollbackDuplicates()\r\n-    .then(() => {\r\n-      console.log('Script completed successfully');\r\n-      process.exit(0);\r\n-    })\r\n-    .catch((error) => {\r\n-      console.error('Script failed:', error);\r\n-      process.exit(1);\r\n-    });\r\n-}\r\n-\r\n export { rollbackDuplicates }; \n\\ No newline at end of file\n"
                },
                {
                    "date": 1739794030814,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n     }, {} as Record<string, string[]>);\r\n \r\n     // Get IDs to delete (all except the first/latest one for each original_id)\r\n     const idsToDelete = Object.values(groupedByOriginalId)\r\n-      .filter(ids => ids.length > 1)\r\n+      .filter((ids: string[]) => ids.length > 1)\r\n       .map(ids => ids.slice(1))\r\n       .flat();\r\n \r\n     if (idsToDelete.length === 0) {\r\n"
                },
                {
                    "date": 1739794037172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n     }, {} as Record<string, string[]>);\r\n \r\n     // Get IDs to delete (all except the first/latest one for each original_id)\r\n     const idsToDelete = Object.values(groupedByOriginalId)\r\n-      .filter((ids: string[]) => ids.length > 1)\r\n+      .filter(ids => ids.length > 1)\r\n       .map(ids => ids.slice(1))\r\n       .flat();\r\n \r\n     if (idsToDelete.length === 0) {\r\n"
                },
                {
                    "date": 1739794050291,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,15 +54,15 @@\n     // Log some sample data to verify structure\r\n     console.log('Sample entries:', duplicates.slice(0, 2));\r\n \r\n     // Group by original_id and keep only the latest entry\r\n-    const groupedByOriginalId = duplicates.reduce((acc, curr) => {\r\n+    const groupedByOriginalId = duplicates.reduce<Record<string, string[]>>((acc, curr) => {\r\n       if (!acc[curr.original_id]) {\r\n         acc[curr.original_id] = [];\r\n       }\r\n       acc[curr.original_id].push(curr.id);\r\n       return acc;\r\n-    }, {} as Record<string, string[]>);\r\n+    }, {});\r\n \r\n     // Get IDs to delete (all except the first/latest one for each original_id)\r\n     const idsToDelete = Object.values(groupedByOriginalId)\r\n       .filter(ids => ids.length > 1)\r\n"
                },
                {
                    "date": 1739794353272,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,8 +14,14 @@\n }\r\n \r\n const supabase = createClient(supabaseUrl, supabaseKey);\r\n \r\n+interface QuestionRecord {\r\n+  id: string;\r\n+  original_id: string;\r\n+  created_at: string;\r\n+}\r\n+\r\n async function rollbackDuplicates() {\r\n   console.log('Starting rollback of duplicate entries...');\r\n \r\n   try {\r\n"
                },
                {
                    "date": 1739794374566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,16 +14,36 @@\n }\r\n \r\n const supabase = createClient(supabaseUrl, supabaseKey);\r\n \r\n+// Terminal colors\r\n+const colors = {\r\n+  reset: '\\x1b[0m',\r\n+  bright: '\\x1b[1m',\r\n+  red: '\\x1b[31m',\r\n+  green: '\\x1b[32m',\r\n+  yellow: '\\x1b[33m',\r\n+  blue: '\\x1b[34m'\r\n+};\r\n+\r\n+// Logger\r\n+const logger = {\r\n+  info: (msg: string) => console.log(`${colors.blue}ℹ ${msg}${colors.reset}`),\r\n+  success: (msg: string) => console.log(`${colors.green}✓ ${msg}${colors.reset}`),\r\n+  warn: (msg: string) => console.log(`${colors.yellow}⚠ ${msg}${colors.reset}`),\r\n+  error: (msg: string) => console.log(`${colors.red}✖ ${msg}${colors.reset}`),\r\n+  divider: () => console.log('\\n' + '─'.repeat(50) + '\\n')\r\n+};\r\n+\r\n interface QuestionRecord {\r\n   id: string;\r\n   original_id: string;\r\n   created_at: string;\r\n }\r\n \r\n async function rollbackDuplicates() {\r\n-  console.log('Starting rollback of duplicate entries...');\r\n+  logger.info('Starting rollback of duplicate entries...');\r\n+  logger.divider();\r\n \r\n   try {\r\n     // Get all questions to check their structure\r\n     const { data: sampleQuestion, error: sampleError } = await supabase\r\n@@ -32,35 +52,35 @@\n       .limit(1)\r\n       .single();\r\n \r\n     if (sampleError) {\r\n-      console.error('Error fetching sample question:', sampleError);\r\n+      logger.error(`Error fetching sample question: ${sampleError.message}`);\r\n       throw sampleError;\r\n     }\r\n \r\n-    console.log('Sample question structure:', sampleQuestion);\r\n+    logger.info('Database connection successful');\r\n+    logger.info(`Sample question ID: ${sampleQuestion.id}`);\r\n+    logger.divider();\r\n \r\n     // Now proceed with duplicate check\r\n     const { data: duplicates, error: findError } = await supabase\r\n       .from('questions')\r\n       .select('*')\r\n       .not('original_id', 'is', null);\r\n \r\n     if (findError) {\r\n-      console.error('Error finding duplicates:', findError);\r\n+      logger.error(`Error finding duplicates: ${findError.message}`);\r\n       throw findError;\r\n     }\r\n \r\n     if (!duplicates || duplicates.length === 0) {\r\n-      console.log('No questions found.');\r\n+      logger.warn('No questions found.');\r\n       return;\r\n     }\r\n \r\n-    console.log(`Total questions found: ${duplicates.length}`);\r\n+    logger.info(`Total questions found: ${duplicates.length}`);\r\n+    logger.divider();\r\n \r\n-    // Log some sample data to verify structure\r\n-    console.log('Sample entries:', duplicates.slice(0, 2));\r\n-\r\n     // Group by original_id and keep only the latest entry\r\n     const groupedByOriginalId = duplicates.reduce<Record<string, string[]>>((acc, curr) => {\r\n       if (!acc[curr.original_id]) {\r\n         acc[curr.original_id] = [];\r\n@@ -75,50 +95,55 @@\n       .map(ids => ids.slice(1))\r\n       .flat();\r\n \r\n     if (idsToDelete.length === 0) {\r\n-      console.log('No duplicates found.');\r\n+      logger.success('No duplicates found.');\r\n       return;\r\n     }\r\n \r\n-    console.log(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n+    logger.warn(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n+    logger.divider();\r\n \r\n     // Delete duplicates in batches of 100\r\n     const batchSize = 100;\r\n     for (let i = 0; i < idsToDelete.length; i += batchSize) {\r\n       const batch = idsToDelete.slice(i, i + batchSize);\r\n-      console.log(`Deleting batch ${i / batchSize + 1}...`);\r\n+      logger.info(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(idsToDelete.length / batchSize)}`);\r\n       \r\n       const { error: deleteError } = await supabase\r\n         .from('questions')\r\n         .delete()\r\n         .in('id', batch);\r\n \r\n       if (deleteError) {\r\n-        console.error('Error deleting batch:', deleteError);\r\n+        logger.error(`Error deleting batch: ${deleteError.message}`);\r\n         throw deleteError;\r\n       }\r\n+      logger.success(`Batch ${Math.floor(i / batchSize) + 1} completed`);\r\n     }\r\n \r\n-    console.log('Successfully removed duplicate entries.');\r\n-    console.log(`Removed ${idsToDelete.length} duplicates.`);\r\n+    logger.divider();\r\n+    logger.success('Rollback completed successfully');\r\n+    logger.success(`Removed ${idsToDelete.length} duplicate entries`);\r\n \r\n   } catch (error) {\r\n-    console.error('Error during rollback:', error);\r\n+    logger.error(`Rollback failed: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n     throw error;\r\n   }\r\n }\r\n \r\n // Execute if this is the main module\r\n if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n-  console.log('Starting script...');\r\n   rollbackDuplicates()\r\n     .then(() => {\r\n-      console.log('Script completed successfully');\r\n+      logger.divider();\r\n+      logger.success('Script completed successfully');\r\n       process.exit(0);\r\n     })\r\n     .catch((error) => {\r\n-      console.error('Script failed:', error);\r\n+      logger.divider();\r\n+      logger.error('Script failed');\r\n+      logger.error(error instanceof Error ? error.message : 'Unknown error');\r\n       process.exit(1);\r\n     });\r\n }\r\n \r\n"
                },
                {
                    "date": 1739794642603,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,19 +2,27 @@\n import dotenv from 'dotenv';\r\n import path from 'path';\r\n import { fileURLToPath } from 'url';\r\n \r\n+console.log('Script started');\r\n+\r\n // Load environment variables\r\n dotenv.config({ path: path.resolve(process.cwd(), '.env') });\r\n \r\n const supabaseUrl = process.env.VITE_SUPABASE_URL;\r\n const supabaseKey = process.env.SUPABASE_SERVICE_KEY;\r\n \r\n+console.log('Environment loaded');\r\n+console.log('Supabase URL:', supabaseUrl ? 'Found' : 'Missing');\r\n+console.log('Supabase Key:', supabaseKey ? 'Found' : 'Missing');\r\n+\r\n if (!supabaseUrl || !supabaseKey) {\r\n-  throw new Error('Missing Supabase environment variables');\r\n+  console.error('Missing environment variables');\r\n+  process.exit(1);\r\n }\r\n \r\n const supabase = createClient(supabaseUrl, supabaseKey);\r\n+console.log('Supabase client created');\r\n \r\n // Terminal colors\r\n const colors = {\r\n   reset: '\\x1b[0m',\r\n@@ -44,8 +52,20 @@\n   logger.info('Starting rollback of duplicate entries...');\r\n   logger.divider();\r\n \r\n   try {\r\n+    const { data, error } = await supabase\r\n+      .from('questions')\r\n+      .select('id, original_id')\r\n+      .limit(1);\r\n+\r\n+    if (error) {\r\n+      logger.error('Database error:', error);\r\n+      throw error;\r\n+    }\r\n+\r\n+    logger.info('Query result:', data);\r\n+    \r\n     // Get all questions to check their structure\r\n     const { data: sampleQuestion, error: sampleError } = await supabase\r\n       .from('questions')\r\n       .select('*')\r\n@@ -130,11 +150,25 @@\n     throw error;\r\n   }\r\n }\r\n \r\n+async function main() {\r\n+  console.log('Starting main function');\r\n+  \r\n+  try {\r\n+    await rollbackDuplicates();\r\n+  } catch (error) {\r\n+    console.error('Caught error:', error);\r\n+    process.exit(1);\r\n+  }\r\n+}\r\n+\r\n+console.log('Executing main function');\r\n+main();\r\n+\r\n // Execute if this is the main module\r\n if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n-  rollbackDuplicates()\r\n+  main()\r\n     .then(() => {\r\n       logger.divider();\r\n       logger.success('Script completed successfully');\r\n       process.exit(0);\r\n"
                },
                {
                    "date": 1739794662578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,9 +58,9 @@\n       .select('id, original_id')\r\n       .limit(1);\r\n \r\n     if (error) {\r\n-      logger.error('Database error:', error);\r\n+      logger.error(error.message);\r\n       throw error;\r\n     }\r\n \r\n     logger.info('Query result:', data);\r\n"
                },
                {
                    "date": 1739794685413,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,9 +62,9 @@\n       logger.error(error.message);\r\n       throw error;\r\n     }\r\n \r\n-    logger.info('Query result:', data);\r\n+    logger.info(JSON.stringify(data, null, 2));\r\n     \r\n     // Get all questions to check their structure\r\n     const { data: sampleQuestion, error: sampleError } = await supabase\r\n       .from('questions')\r\n"
                },
                {
                    "date": 1739794742489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n     // Now proceed with duplicate check\r\n     const { data: duplicates, error: findError } = await supabase\r\n       .from('questions')\r\n       .select('*')\r\n-      .not('original_id', 'is', null);\r\n+      .order('created_at', { ascending: false });\r\n \r\n     if (findError) {\r\n       logger.error(`Error finding duplicates: ${findError.message}`);\r\n       throw findError;\r\n@@ -99,19 +99,20 @@\n \r\n     logger.info(`Total questions found: ${duplicates.length}`);\r\n     logger.divider();\r\n \r\n-    // Group by original_id and keep only the latest entry\r\n-    const groupedByOriginalId = duplicates.reduce<Record<string, string[]>>((acc, curr) => {\r\n-      if (!acc[curr.original_id]) {\r\n-        acc[curr.original_id] = [];\r\n+    // Group by content to find duplicates\r\n+    const groupedByContent = duplicates.reduce<Record<string, string[]>>((acc, curr) => {\r\n+      const key = `${curr.title}-${curr.content}`;\r\n+      if (!acc[key]) {\r\n+        acc[key] = [];\r\n       }\r\n-      acc[curr.original_id].push(curr.id);\r\n+      acc[key].push(curr.id);\r\n       return acc;\r\n     }, {});\r\n \r\n     // Get IDs to delete (all except the first/latest one for each original_id)\r\n-    const idsToDelete = Object.values(groupedByOriginalId)\r\n+    const idsToDelete = Object.values(groupedByContent)\r\n       .filter(ids => ids.length > 1)\r\n       .map(ids => ids.slice(1))\r\n       .flat();\r\n \r\n"
                },
                {
                    "date": 1739908855525,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,8 +52,34 @@\n   logger.info('Starting rollback of duplicate entries...');\r\n   logger.divider();\r\n \r\n   try {\r\n+    // First handle questions duplicates\r\n+    await handleQuestionDuplicates();\r\n+\r\n+    // Handle topic duplicates\r\n+    await handleTopicDuplicates();\r\n+\r\n+    // Handle subtopic duplicates\r\n+    await handleSubtopicDuplicates();\r\n+\r\n+    // Clean up empty lessons\r\n+    await cleanupEmptyLessons();\r\n+\r\n+    logger.divider();\r\n+    logger.success('Rollback completed successfully');\r\n+\r\n+  } catch (error) {\r\n+    logger.error(`Rollback failed: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n+    throw error;\r\n+  }\r\n+}\r\n+\r\n+async function handleQuestionDuplicates() {\r\n+  logger.info('Starting rollback of duplicate questions...');\r\n+  logger.divider();\r\n+\r\n+  try {\r\n     const { data, error } = await supabase\r\n       .from('questions')\r\n       .select('id, original_id')\r\n       .limit(1);\r\n@@ -151,8 +177,147 @@\n     throw error;\r\n   }\r\n }\r\n \r\n+async function handleTopicDuplicates() {\r\n+  logger.info('Checking for duplicate topics...');\r\n+\r\n+  const { data: topics, error: topicsError } = await supabase\r\n+    .from('topics')\r\n+    .select('*')\r\n+    .order('created_at', { ascending: false });\r\n+\r\n+  if (topicsError) {\r\n+    logger.error(`Error fetching topics: ${topicsError.message}`);\r\n+    throw topicsError;\r\n+  }\r\n+\r\n+  // Group by title to find duplicates\r\n+  const groupedByTitle = topics.reduce<Record<string, any[]>>((acc, curr) => {\r\n+    const key = curr.title.toLowerCase().trim();\r\n+    if (!acc[key]) acc[key] = [];\r\n+    acc[key].push(curr);\r\n+    return acc;\r\n+  }, {});\r\n+\r\n+  // Get IDs to delete (keep the most recent one)\r\n+  const topicsToDelete = Object.values(groupedByTitle)\r\n+    .filter(group => group.length > 1)\r\n+    .map(group => group.slice(1))\r\n+    .flat()\r\n+    .map(topic => topic.id);\r\n+\r\n+  if (topicsToDelete.length > 0) {\r\n+    logger.warn(`Found ${topicsToDelete.length} duplicate topics to remove`);\r\n+    \r\n+    // Delete in batches\r\n+    const batchSize = 100;\r\n+    for (let i = 0; i < topicsToDelete.length; i += batchSize) {\r\n+      const batch = topicsToDelete.slice(i, i + batchSize);\r\n+      const { error: deleteError } = await supabase\r\n+        .from('topics')\r\n+        .delete()\r\n+        .in('id', batch);\r\n+\r\n+      if (deleteError) {\r\n+        logger.error(`Error deleting topic batch: ${deleteError.message}`);\r\n+        throw deleteError;\r\n+      }\r\n+    }\r\n+    logger.success(`Removed ${topicsToDelete.length} duplicate topics`);\r\n+  } else {\r\n+    logger.success('No duplicate topics found');\r\n+  }\r\n+}\r\n+\r\n+async function handleSubtopicDuplicates() {\r\n+  logger.info('Checking for duplicate subtopics...');\r\n+\r\n+  const { data: subtopics, error: subtopicsError } = await supabase\r\n+    .from('subtopics')\r\n+    .select('*')\r\n+    .order('created_at', { ascending: false });\r\n+\r\n+  if (subtopicsError) {\r\n+    logger.error(`Error fetching subtopics: ${subtopicsError.message}`);\r\n+    throw subtopicsError;\r\n+  }\r\n+\r\n+  // Group by title within same topic to find duplicates\r\n+  const groupedByTitleAndTopic = subtopics.reduce<Record<string, any[]>>((acc, curr) => {\r\n+    const key = `${curr.topic_id}-${curr.title.toLowerCase().trim()}`;\r\n+    if (!acc[key]) acc[key] = [];\r\n+    acc[key].push(curr);\r\n+    return acc;\r\n+  }, {});\r\n+\r\n+  // Get IDs to delete (keep the most recent one)\r\n+  const subtopicsToDelete = Object.values(groupedByTitleAndTopic)\r\n+    .filter(group => group.length > 1)\r\n+    .map(group => group.slice(1))\r\n+    .flat()\r\n+    .map(subtopic => subtopic.id);\r\n+\r\n+  if (subtopicsToDelete.length > 0) {\r\n+    logger.warn(`Found ${subtopicsToDelete.length} duplicate subtopics to remove`);\r\n+    \r\n+    // Delete in batches\r\n+    const batchSize = 100;\r\n+    for (let i = 0; i < subtopicsToDelete.length; i += batchSize) {\r\n+      const batch = subtopicsToDelete.slice(i, i + batchSize);\r\n+      const { error: deleteError } = await supabase\r\n+        .from('subtopics')\r\n+        .delete()\r\n+        .in('id', batch);\r\n+\r\n+      if (deleteError) {\r\n+        logger.error(`Error deleting subtopic batch: ${deleteError.message}`);\r\n+        throw deleteError;\r\n+      }\r\n+    }\r\n+    logger.success(`Removed ${subtopicsToDelete.length} duplicate subtopics`);\r\n+  } else {\r\n+    logger.success('No duplicate subtopics found');\r\n+  }\r\n+}\r\n+\r\n+async function cleanupEmptyLessons() {\r\n+  logger.info('Checking for empty lessons...');\r\n+\r\n+  const { data: lessons, error: lessonsError } = await supabase\r\n+    .from('lessons')\r\n+    .select('*, questions!inner(*)');\r\n+\r\n+  if (lessonsError) {\r\n+    logger.error(`Error fetching lessons: ${lessonsError.message}`);\r\n+    throw lessonsError;\r\n+  }\r\n+\r\n+  // Find lessons with no questions\r\n+  const emptyLessons = lessons.filter(lesson => !lesson.questions || lesson.questions.length === 0);\r\n+\r\n+  if (emptyLessons.length > 0) {\r\n+    logger.warn(`Found ${emptyLessons.length} empty lessons to remove`);\r\n+    \r\n+    const emptyLessonIds = emptyLessons.map(lesson => lesson.id);\r\n+    \r\n+    // Delete empty lessons\r\n+    const { error: deleteError } = await supabase\r\n+      .from('lessons')\r\n+      .delete()\r\n+      .in('id', emptyLessonIds);\r\n+\r\n+    if (deleteError) {\r\n+      logger.error(`Error deleting empty lessons: ${deleteError.message}`);\r\n+      throw deleteError;\r\n+    }\r\n+    \r\n+    logger.success(`Removed ${emptyLessons.length} empty lessons`);\r\n+  } else {\r\n+    logger.success('No empty lessons found');\r\n+  }\r\n+}\r\n+\r\n async function main() {\r\n   console.log('Starting main function');\r\n   \r\n   try {\r\n"
                },
                {
                    "date": 1739908967232,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,11 +180,15 @@\n \r\n async function handleTopicDuplicates() {\r\n   logger.info('Checking for duplicate topics...');\r\n \r\n+  // First get all topics with their associated lessons\r\n   const { data: topics, error: topicsError } = await supabase\r\n     .from('topics')\r\n-    .select('*')\r\n+    .select(`\r\n+      *,\r\n+      lessons:lessons(*)\r\n+    `)\r\n     .order('created_at', { ascending: false });\r\n \r\n   if (topicsError) {\r\n     logger.error(`Error fetching topics: ${topicsError.message}`);\r\n@@ -198,44 +202,75 @@\n     acc[key].push(curr);\r\n     return acc;\r\n   }, {});\r\n \r\n-  // Get IDs to delete (keep the most recent one)\r\n-  const topicsToDelete = Object.values(groupedByTitle)\r\n+  // Get duplicate topics (keeping the most recent one)\r\n+  const duplicateTopics = Object.values(groupedByTitle)\r\n     .filter(group => group.length > 1)\r\n     .map(group => group.slice(1))\r\n-    .flat()\r\n-    .map(topic => topic.id);\r\n+    .flat();\r\n \r\n-  if (topicsToDelete.length > 0) {\r\n-    logger.warn(`Found ${topicsToDelete.length} duplicate topics to remove`);\r\n-    \r\n-    // Delete in batches\r\n-    const batchSize = 100;\r\n-    for (let i = 0; i < topicsToDelete.length; i += batchSize) {\r\n-      const batch = topicsToDelete.slice(i, i + batchSize);\r\n-      const { error: deleteError } = await supabase\r\n+  if (duplicateTopics.length > 0) {\r\n+    logger.warn(`Found ${duplicateTopics.length} duplicate topics to process`);\r\n+\r\n+    // First, delete associated empty lessons\r\n+    for (const topic of duplicateTopics) {\r\n+      if (topic.lessons && topic.lessons.length > 0) {\r\n+        const lessonIds = topic.lessons.map((lesson: any) => lesson.id);\r\n+        \r\n+        // Delete lessons associated with this topic\r\n+        const { error: deleteError } = await supabase\r\n+          .from('lessons')\r\n+          .delete()\r\n+          .in('id', lessonIds);\r\n+\r\n+        if (deleteError) {\r\n+          logger.error(`Error deleting lessons for topic ${topic.id}: ${deleteError.message}`);\r\n+          continue;\r\n+        }\r\n+        logger.success(`Removed ${lessonIds.length} lessons from topic ${topic.id}`);\r\n+      }\r\n+\r\n+      // Delete subtopics associated with this topic\r\n+      const { error: subtopicsError } = await supabase\r\n+        .from('subtopics')\r\n+        .delete()\r\n+        .eq('topic_id', topic.id);\r\n+\r\n+      if (subtopicsError) {\r\n+        logger.error(`Error deleting subtopics for topic ${topic.id}: ${subtopicsError.message}`);\r\n+        continue;\r\n+      }\r\n+\r\n+      // Finally delete the topic\r\n+      const { error: topicError } = await supabase\r\n         .from('topics')\r\n         .delete()\r\n-        .in('id', batch);\r\n+        .eq('id', topic.id);\r\n \r\n-      if (deleteError) {\r\n-        logger.error(`Error deleting topic batch: ${deleteError.message}`);\r\n-        throw deleteError;\r\n+      if (topicError) {\r\n+        logger.error(`Error deleting topic ${topic.id}: ${topicError.message}`);\r\n+        continue;\r\n       }\r\n+      logger.success(`Successfully removed duplicate topic: ${topic.title}`);\r\n     }\r\n-    logger.success(`Removed ${topicsToDelete.length} duplicate topics`);\r\n+\r\n+    logger.success(`Completed processing ${duplicateTopics.length} duplicate topics`);\r\n   } else {\r\n     logger.success('No duplicate topics found');\r\n   }\r\n }\r\n \r\n async function handleSubtopicDuplicates() {\r\n   logger.info('Checking for duplicate subtopics...');\r\n \r\n+  // Get subtopics with their associated lessons\r\n   const { data: subtopics, error: subtopicsError } = await supabase\r\n     .from('subtopics')\r\n-    .select('*')\r\n+    .select(`\r\n+      *,\r\n+      lessons:lessons(*)\r\n+    `)\r\n     .order('created_at', { ascending: false });\r\n \r\n   if (subtopicsError) {\r\n     logger.error(`Error fetching subtopics: ${subtopicsError.message}`);\r\n@@ -249,33 +284,48 @@\n     acc[key].push(curr);\r\n     return acc;\r\n   }, {});\r\n \r\n-  // Get IDs to delete (keep the most recent one)\r\n-  const subtopicsToDelete = Object.values(groupedByTitleAndTopic)\r\n+  // Get duplicate subtopics\r\n+  const duplicateSubtopics = Object.values(groupedByTitleAndTopic)\r\n     .filter(group => group.length > 1)\r\n     .map(group => group.slice(1))\r\n-    .flat()\r\n-    .map(subtopic => subtopic.id);\r\n+    .flat();\r\n \r\n-  if (subtopicsToDelete.length > 0) {\r\n-    logger.warn(`Found ${subtopicsToDelete.length} duplicate subtopics to remove`);\r\n-    \r\n-    // Delete in batches\r\n-    const batchSize = 100;\r\n-    for (let i = 0; i < subtopicsToDelete.length; i += batchSize) {\r\n-      const batch = subtopicsToDelete.slice(i, i + batchSize);\r\n-      const { error: deleteError } = await supabase\r\n+  if (duplicateSubtopics.length > 0) {\r\n+    logger.warn(`Found ${duplicateSubtopics.length} duplicate subtopics to process`);\r\n+\r\n+    for (const subtopic of duplicateSubtopics) {\r\n+      // First delete associated lessons\r\n+      if (subtopic.lessons && subtopic.lessons.length > 0) {\r\n+        const lessonIds = subtopic.lessons.map((lesson: any) => lesson.id);\r\n+        \r\n+        const { error: deleteError } = await supabase\r\n+          .from('lessons')\r\n+          .delete()\r\n+          .in('id', lessonIds);\r\n+\r\n+        if (deleteError) {\r\n+          logger.error(`Error deleting lessons for subtopic ${subtopic.id}: ${deleteError.message}`);\r\n+          continue;\r\n+        }\r\n+        logger.success(`Removed ${lessonIds.length} lessons from subtopic ${subtopic.id}`);\r\n+      }\r\n+\r\n+      // Then delete the subtopic\r\n+      const { error: subtopicError } = await supabase\r\n         .from('subtopics')\r\n         .delete()\r\n-        .in('id', batch);\r\n+        .eq('id', subtopic.id);\r\n \r\n-      if (deleteError) {\r\n-        logger.error(`Error deleting subtopic batch: ${deleteError.message}`);\r\n-        throw deleteError;\r\n+      if (subtopicError) {\r\n+        logger.error(`Error deleting subtopic ${subtopic.id}: ${subtopicError.message}`);\r\n+        continue;\r\n       }\r\n+      logger.success(`Successfully removed duplicate subtopic: ${subtopic.title}`);\r\n     }\r\n-    logger.success(`Removed ${subtopicsToDelete.length} duplicate subtopics`);\r\n+\r\n+    logger.success(`Completed processing ${duplicateSubtopics.length} duplicate subtopics`);\r\n   } else {\r\n     logger.success('No duplicate subtopics found');\r\n   }\r\n }\r\n"
                },
                {
                    "date": 1739909289418,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n import { createClient } from '@supabase/supabase-js';\r\n import dotenv from 'dotenv';\r\n import path from 'path';\r\n import { fileURLToPath } from 'url';\r\n+import fs from 'fs/promises';\r\n \r\n console.log('Script started');\r\n \r\n // Load environment variables\r\n@@ -47,8 +48,51 @@\n   original_id: string;\r\n   created_at: string;\r\n }\r\n \r\n+// Backup directory setup\r\n+const BACKUP_DIR = path.join(process.cwd(), 'backups');\r\n+const getBackupPath = (type: string) => path.join(BACKUP_DIR, `${type}_${new Date().toISOString().replace(/[:.]/g, '-')}.json`);\r\n+\r\n+// Backup functionality\r\n+async function createBackup(type: string, data: any[]) {\r\n+  try {\r\n+    await fs.mkdir(BACKUP_DIR, { recursive: true });\r\n+    const backupPath = getBackupPath(type);\r\n+    await fs.writeFile(backupPath, JSON.stringify(data, null, 2));\r\n+    logger.success(`Backup created: ${backupPath}`);\r\n+    return backupPath;\r\n+  } catch (error) {\r\n+    logger.error(`Backup failed: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n+    throw error;\r\n+  }\r\n+}\r\n+\r\n+// Restore functionality\r\n+async function restoreFromBackup(backupPath: string) {\r\n+  try {\r\n+    const backupData = JSON.parse(await fs.readFile(backupPath, 'utf-8'));\r\n+    const type = path.basename(backupPath).split('_')[0];\r\n+\r\n+    logger.info(`Restoring ${type} from backup: ${backupPath}`);\r\n+\r\n+    for (const item of backupData) {\r\n+      const { error } = await supabase\r\n+        .from(type)\r\n+        .upsert(item, { onConflict: 'id' });\r\n+\r\n+      if (error) {\r\n+        logger.error(`Error restoring item ${item.id}: ${error.message}`);\r\n+      }\r\n+    }\r\n+\r\n+    logger.success(`Restored ${backupData.length} items from backup`);\r\n+  } catch (error) {\r\n+    logger.error(`Restore failed: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n+    throw error;\r\n+  }\r\n+}\r\n+\r\n async function rollbackDuplicates() {\r\n   logger.info('Starting rollback of duplicate entries...');\r\n   logger.divider();\r\n \r\n@@ -180,22 +224,36 @@\n \r\n async function handleTopicDuplicates() {\r\n   logger.info('Checking for duplicate topics...');\r\n \r\n-  // First get all topics with their associated lessons\r\n   const { data: topics, error: topicsError } = await supabase\r\n     .from('topics')\r\n     .select(`\r\n       *,\r\n-      lessons:lessons(*)\r\n+      lessons:lessons(*),\r\n+      subtopics:subtopics(*)\r\n     `)\r\n     .order('created_at', { ascending: false });\r\n \r\n-  if (topicsError) {\r\n-    logger.error(`Error fetching topics: ${topicsError.message}`);\r\n-    throw topicsError;\r\n-  }\r\n+  if (topicsError) throw topicsError;\r\n \r\n+  // Create backups before deletion\r\n+  const backupPaths = {\r\n+    topics: await createBackup('topics', topics),\r\n+    lessons: await createBackup('lessons', topics.flatMap(t => t.lessons || [])),\r\n+    subtopics: await createBackup('subtopics', topics.flatMap(t => t.subtopics || []))\r\n+  };\r\n+\r\n+  // Store backup metadata\r\n+  await fs.writeFile(\r\n+    path.join(BACKUP_DIR, 'backup_metadata.json'),\r\n+    JSON.stringify({\r\n+      timestamp: new Date().toISOString(),\r\n+      paths: backupPaths,\r\n+      type: 'topic_cleanup'\r\n+    })\r\n+  );\r\n+\r\n   // Group by title to find duplicates\r\n   const groupedByTitle = topics.reduce<Record<string, any[]>>((acc, curr) => {\r\n     const key = curr.title.toLowerCase().trim();\r\n     if (!acc[key]) acc[key] = [];\r\n@@ -366,22 +424,73 @@\n     logger.success('No empty lessons found');\r\n   }\r\n }\r\n \r\n-async function main() {\r\n-  console.log('Starting main function');\r\n+// Restore command\r\n+async function restoreBackup(timestamp?: string) {\r\n+  const metadataPath = path.join(BACKUP_DIR, 'backup_metadata.json');\r\n   \r\n   try {\r\n-    await rollbackDuplicates();\r\n+    const metadata = JSON.parse(await fs.readFile(metadataPath, 'utf-8'));\r\n+    \r\n+    if (timestamp && metadata.timestamp !== timestamp) {\r\n+      throw new Error('Backup timestamp does not match');\r\n+    }\r\n+\r\n+    // Restore in reverse order to handle foreign keys\r\n+    await restoreFromBackup(metadata.paths.lessons);\r\n+    await restoreFromBackup(metadata.paths.subtopics);\r\n+    await restoreFromBackup(metadata.paths.topics);\r\n+\r\n+    logger.success('Restore completed successfully');\r\n   } catch (error) {\r\n-    console.error('Caught error:', error);\r\n-    process.exit(1);\r\n+    logger.error(`Restore failed: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n+    throw error;\r\n   }\r\n }\r\n \r\n-console.log('Executing main function');\r\n-main();\r\n+// List available backups\r\n+async function listBackups() {\r\n+  try {\r\n+    const files = await fs.readdir(BACKUP_DIR);\r\n+    const metadataFiles = files.filter(f => f.endsWith('metadata.json'));\r\n+    \r\n+    for (const file of metadataFiles) {\r\n+      const metadata = JSON.parse(\r\n+        await fs.readFile(path.join(BACKUP_DIR, file), 'utf-8')\r\n+      );\r\n+      console.log(`Backup from ${metadata.timestamp} (${metadata.type})`);\r\n+    }\r\n+  } catch (error) {\r\n+    logger.error(`Failed to list backups: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n+  }\r\n+}\r\n \r\n+// Modified main function with command handling\r\n+async function main() {\r\n+  const command = process.argv[2];\r\n+  const timestamp = process.argv[3];\r\n+\r\n+  switch (command) {\r\n+    case 'backup':\r\n+      await handleTopicDuplicates();\r\n+      break;\r\n+    case 'restore':\r\n+      await restoreBackup(timestamp);\r\n+      break;\r\n+    case 'list':\r\n+      await listBackups();\r\n+      break;\r\n+    default:\r\n+      console.log(`\r\n+Usage:\r\n+  npx ts-node src/scripts/rollback-duplicates.ts backup    # Create backup and remove duplicates\r\n+  npx ts-node src/scripts/rollback-duplicates.ts restore [timestamp]  # Restore from backup\r\n+  npx ts-node src/scripts/rollback-duplicates.ts list     # List available backups\r\n+      `);\r\n+  }\r\n+}\r\n+\r\n // Execute if this is the main module\r\n if (import.meta.url === fileURLToPath(import.meta.url)) {\r\n   main()\r\n     .then(() => {\r\n@@ -396,5 +505,5 @@\n       process.exit(1);\r\n     });\r\n }\r\n \r\n-export { rollbackDuplicates }; \n\\ No newline at end of file\n+export { rollbackDuplicates, restoreBackup, listBackups }; \n\\ No newline at end of file\n"
                },
                {
                    "date": 1739909303188,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -224,8 +224,9 @@\n \r\n async function handleTopicDuplicates() {\r\n   logger.info('Checking for duplicate topics...');\r\n \r\n+  // First get all topics with their associated lessons\r\n   const { data: topics, error: topicsError } = await supabase\r\n     .from('topics')\r\n     .select(`\r\n       *,\r\n"
                },
                {
                    "date": 1739909500961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -465,9 +465,9 @@\n     logger.error(`Failed to list backups: ${error instanceof Error ? error.message : 'Unknown error'}`);\r\n   }\r\n }\r\n \r\n-// Modified main function with command handling\r\n+// Modified main function to include the actual rollback\r\n async function main() {\r\n   const command = process.argv[2];\r\n   const timestamp = process.argv[3];\r\n \r\n@@ -480,14 +480,18 @@\n       break;\r\n     case 'list':\r\n       await listBackups();\r\n       break;\r\n+    case 'rollback':\r\n+      await rollbackDuplicates();\r\n+      break;\r\n     default:\r\n       console.log(`\r\n Usage:\r\n-  npx ts-node src/scripts/rollback-duplicates.ts backup    # Create backup and remove duplicates\r\n+  npx ts-node src/scripts/rollback-duplicates.ts rollback   # Remove duplicates with backup\r\n+  npx ts-node src/scripts/rollback-duplicates.ts backup     # Only create backup\r\n   npx ts-node src/scripts/rollback-duplicates.ts restore [timestamp]  # Restore from backup\r\n-  npx ts-node src/scripts/rollback-duplicates.ts list     # List available backups\r\n+  npx ts-node src/scripts/rollback-duplicates.ts list      # List available backups\r\n       `);\r\n   }\r\n }\r\n \r\n"
                },
                {
                    "date": 1740925815767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n   error: (msg: string) => console.log(`${colors.red}✖ ${msg}${colors.reset}`),\r\n   divider: () => console.log('\\n' + '─'.repeat(50) + '\\n')\r\n };\r\n \r\n-interface QuestionRecord {\r\n+export interface QuestionRecord {\r\n   id: string;\r\n   original_id: string;\r\n   created_at: string;\r\n }\r\n"
                }
            ],
            "date": 1739791839926,
            "name": "Commit-0",
            "content": "import { createClient } from '@supabase/supabase-js';\r\nimport dotenv from 'dotenv';\r\nimport path from 'path';\r\n\r\n// Load environment variables\r\ndotenv.config({ path: path.resolve(process.cwd(), '.env') });\r\n\r\nconst supabaseUrl = process.env.VITE_SUPABASE_URL;\r\nconst supabaseKey = process.env.SUPABASE_SERVICE_KEY;\r\n\r\nif (!supabaseUrl || !supabaseKey) {\r\n  throw new Error('Missing Supabase environment variables');\r\n}\r\n\r\nconst supabase = createClient(supabaseUrl, supabaseKey);\r\n\r\nasync function rollbackDuplicates() {\r\n  console.log('Starting rollback of duplicate entries...');\r\n\r\n  try {\r\n    // First, identify duplicates by original_id\r\n    const { data: duplicates, error: findError } = await supabase\r\n      .from('questions')\r\n      .select('original_id, id, created_at')\r\n      .not('original_id', 'is', null)\r\n      .order('created_at', { ascending: false });\r\n\r\n    if (findError) throw findError;\r\n\r\n    // Group by original_id and keep only the latest entry\r\n    const groupedByOriginalId = duplicates?.reduce((acc, curr) => {\r\n      if (!acc[curr.original_id]) {\r\n        acc[curr.original_id] = [];\r\n      }\r\n      acc[curr.original_id].push(curr.id);\r\n      return acc;\r\n    }, {} as Record<string, string[]>);\r\n\r\n    // Get IDs to delete (all except the first/latest one for each original_id)\r\n    const idsToDelete = Object.values(groupedByOriginalId)\r\n      .filter(ids => ids.length > 1)\r\n      .map(ids => ids.slice(1))\r\n      .flat();\r\n\r\n    if (idsToDelete.length === 0) {\r\n      console.log('No duplicates found.');\r\n      return;\r\n    }\r\n\r\n    console.log(`Found ${idsToDelete.length} duplicate entries to remove.`);\r\n\r\n    // Delete duplicates\r\n    const { error: deleteError } = await supabase\r\n      .from('questions')\r\n      .delete()\r\n      .in('id', idsToDelete);\r\n\r\n    if (deleteError) throw deleteError;\r\n\r\n    console.log('Successfully removed duplicate entries.');\r\n    console.log(`Removed ${idsToDelete.length} duplicates.`);\r\n\r\n  } catch (error) {\r\n    console.error('Error during rollback:', error);\r\n    throw error;\r\n  }\r\n}\r\n\r\n// Execute if running directly\r\nif (require.main === module) {\r\n  rollbackDuplicates()\r\n    .then(() => process.exit(0))\r\n    .catch((error) => {\r\n      console.error(error);\r\n      process.exit(1);\r\n    });\r\n}\r\n\r\nexport { rollbackDuplicates }; "
        }
    ]
}